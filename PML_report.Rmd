---
title: "Quantifying Accuracy of Weight Lifting Exercise"
output: html_document
---


### Executive Summary

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: <http://groupware.les.inf.puc-rio.br/har> (see the section on the Weight Lifting Exercise Dataset). 


####Data 

The training data for this project are available here: [<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>]

The test data are available here: [<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>]

The data for this project come from this source: [<http://groupware.les.inf.puc-rio.br/har>]. 


#### Model building 
The outcome variable is "classe", a factor variable with five levels:

- exactly according to the specification (Class A)
- throwing the elbows to the front (Class B)
- lifting the dumbbell only halfway (Class C)
- lowering the dumbbell only halfway (Class D)
- throwing the hips to the front (Class E)

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes.

We will use two modeling building techiniques to test our data using decision trees and random forests. Which ever techinique provides the highest accuracy for our model that will be our final model. 

####Cross-Validation 
Cross-validation will be performed by splitting our training data set randomly without replacement into two data sets: a training and testing set. 70% of our training data will be using for the training set and the remaining 30% for our testing set. Our models will be fitted on the training data set, and evaluated on the testing set. Once the most accurate model is choosen, it will be tested on the original testing data set.

####Expected out-of-sample error

The expected out of sample error will correspond to the quantity: 1-accuracy in the cross-validation data. Accuracy is the proportion of correct classified observation over the total sample in the testing data set. Expected accuracy is the expected accuracy in the out-of-sample data set (i.e. original testing data set). 

### Data Analysis

Here we are going to load the necessary packages for data analysis.
```{r, packages}
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(rattle)
```

#####Loading and cleaning data
We are going to load the data into R and clean the data for usability.
```{r, datainput}
## Set working directory
setwd('/Users/loperbk/Desktop/Coursera/Practical Machine Learning')

# Some missing values are coded as string "#DIV/0!" or "" or "NA" - these will be changed to NA.
# We notice that both data sets contain columns with all missing values - these will be deleted.
trainingData <- read.csv(file="pml-training.csv", na.strings=c('NA','','#DIV/0!'))
testingData <- read.csv(file="pml-testing.csv", na.strings=c('NA','','#DIV/0!'))

## Removing NAs from data
rNAs <- apply(trainingData,2,function(x){sum(is.na(x))}) ## apply to columns
trainingData <- trainingData[,which(rNAs == 0)]

rNAs <- apply(testingData,2,function(x){sum(is.na(x))}) 
testingData <- testingData[,which(rNAs == 0)]
## Removing columns we dont need 
trainingData   <-trainingData[,-c(1:7)]
testingData <-testingData[,-c(1:7)]
```

#####Create our training and test sets.
Since we have a relatively large data set we are going to partition our data into two data sets: training set with 70% and the testing set with 30%. 
```{r}
set.seed(90059) ## set seed for reproducibility
trainIndex = createDataPartition(y = trainingData$classe, p = 0.70, list=FALSE)
training = trainingData[trainIndex,]
testing = trainingData[-trainIndex,]
dim(training);dim(testing) ## 5885 examples in my training set to use in my prediction model
```

#####Create a model fit: Using Decision Tree
```{r, rpart}
modFit1 <- rpart(classe ~ ., data=training, method="class")
fancyRpartPlot(modFit1)

## Predicting
##Evalute our model using the test set. 
predictions <- predict(modFit1, newdata=testing, type = "class")
confusionMatrix(predictions, testing$classe) ## Accuracy 
```
We see here that the decision tree is relatively good with 0.726 accuracy. So our out of sample is 1-accuracy for the prediction  against the testing set is 1-0.726 = 0.274, which is relatively on the high side. However, we are going to use another method, random forest to see if this gives us better accuracy. Based on what we know random forest will give a greater accuracy for prediciton. Let's take a look.

#####Create a model fit: Using Random Forest
```{r, randomforest}
modFit2 <- randomForest(classe ~ ., data=training)

predictions2 <- predict(modFit2, testing, type = "class")
confusionMatrix(predictions2, testing$classe)
```

#####Final Model
Here we see that random forest give a much higher accuracy of 0.996. So we will use the *random forest model*. The expected out of sample error to be calculated is 1-accuracy for the prediction against the testing set  which is 1-0.996 = 0.004 or 0.4%. 


### Application of Random Forest Algorithm 
We are using our prediction model to predict 20 different test cases. 
```{r, answers}
pred <- predict(modFit2, testingData, type = "class")
pml_write_files = function(x){
        n = length(x)
        for(i in 1:n){
                filename = paste0("problem_id_",i,".txt")
                write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
        }
}

pml_write_files(pred) ## write the prediction to a folder 
pred